{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression Analysis\n",
    "\n",
    "*Written by Jin Cheong & Luke Chang*\n",
    "\n",
    "In this lab we are going to learn how to do simple data analyses using python. \n",
    "This module includes learning how to check data, drawing plots for visualization, \n",
    "running simple linear regression models, comparing models, & visualizing fits. - This notebook was adapted from material available [here](https://github.com/justmarkham/DAT4/blob/master/notebooks/08_linear_regression.ipynb)\n",
    "\n",
    "After the tutorial you will have the chance to apply the methods to a new set of data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Data Checks\n",
    "\n",
    "Preliminary data checks are important because real life data are rarely clean. \n",
    "There are missing values, outliers, miscoded values, and other errors.\n",
    "In this section we will learn how to detect these errors and prepare our dataset for analysis.\n",
    "\n",
    "We first load the modules that we will be using for this section.  Numpy is akin to Matlab as it can represent n-dimensional matrices and provides lots of useful functions for manipulating data.  Pandas is akin to a data frame in R and provides an intuitive way to interact with data in a 2D data frame.  Matplotlib is a standard plotting library that is similar in functionality to Matlab's object oriented plotting.  Statsmodels is a library for performing various statistical analyses such as linear regression.\n",
    "\n",
    "```\n",
    "%matplotlib inline\n",
    "```\n",
    "is Jupyter notebook cell magic and allows us to render matplotlib plots within the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will load the data which is a comma delimited text file using the `read_csv()` method from pandas. \n",
    "The '../Data' indicates that we will move one folder up and then into the Data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df = pd.read_csv('../Data/salary.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is easy to see which variables are currently loaded into your python environment using the `whos` command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "whos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can return the data frame by simply calling it in a cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, often the dataframes can be large and we may be only interested in seeing the first few rows.  `df.head()` is useful for this purpose.  `shape` is another useful method for getting the dimensions of the matrix.  We will print the number of rows and columns in this data set by using output formatting.  Use the `%` sign to indicate the type of data (e.g., `%i`=integer, `%d`=float, `%s`=string), then use the `%` followed by a tuple of the values you would like to insert into the text.  See [here](https://pyformat.info/) for more info about formatting text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'There are %i rows and %i columns in this data set' % df.shape\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is easy to quickly count the number of missing values for each column in the dataset using the `isnull()` method.  One thing that is really nice about Python is that you can chain commands, which means that the output of one method can be the input into the next method.  This allows us to write intuitive and concise code.  Notice how we take the `sum()` of all of the null cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are different techniques for dealing with missing data.  An easy one is to simply remove rows that have any missing values using the `dropna()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can check to make sure the missing rows are removed.  Let's also check the new dimensions of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'There are %i rows and %i columns in this data set' % df.shape\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `describe()` method to get a quick summary of the continuous values of the data frame. We will `transpose()` the output to make it slightly easier to read. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get quick summary of a pandas series, or specific column of a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.departm.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several ways to quickly look at the column names.  One is using the `columns` method.  Another is calling the `keys()` of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print df.columns\n",
    "print df.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A really nice feature of Pandas is that chaining methods also includes plotting functions.  Here we will first subset the dataframe using a list of the columns we want to retain (e.g., Salary and Gender).  Next we will create a boxplot to see if there is a difference in salary as a function of gender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ax = df[['salary','gender']].boxplot(by='gender')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that there are 3 levels.  This probably means that at least one participant did not indicate their gender or selected other.  Let's exclude this datapoint and recreate the plot.  We can input a logical value of rows, and a list of the number of columns using the `.ix` call to the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.ix[df['gender']!=2,['salary','gender']].boxplot(by='gender')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code was written concisely as a one liner to omit other gender but to actually subset the data, you'd need to reassign the dataframe. Then we check how many males (gender = 0) and females (gender = 1) are in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = df[df['gender']!=2]  \n",
    "df.groupby('gender').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also create scatterplots to examine a relationship between two continuous variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df[['salary','years']].plot(kind='scatter', x='years', y='salary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Oneshot visualization\n",
    "## create a new numericalSeries called dept !! Just for visualization !! \n",
    "df['dept_num'] = df.departm.map({'bio':0, 'chem':1,'geol':2,'neuro':3,'stat':4,'physics':5,'math':6})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Now plot all four categories\n",
    "fig, axs = plt.subplots(1, 4, sharey=True)\n",
    "df.plot(kind='scatter', x='gender', y='salary', ax=axs[0], figsize=(16, 8))\n",
    "df.plot(kind='scatter', x='dept_num', y='salary', ax=axs[1])\n",
    "df.plot(kind='scatter', x='years', y='salary', ax=axs[2])\n",
    "df.plot(kind='scatter', x='age', y='salary', ax=axs[3])\n",
    "# The problem is that it treats department as a continuous variable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "Now let's move on to running an analysis.\n",
    "\n",
    "Simple linear regression is an approach for predicting a **quantitative response** using a **single feature** (or \"predictor\" or \"input variable\"). It takes the following form:\n",
    "\n",
    "$y = \\beta_0 + \\beta_1\\cdot x $\n",
    "\n",
    "What does each term represent?\n",
    "- $y$ is the response\n",
    "- $x$ is the feature\n",
    "- $\\beta_0$ is the intercept\n",
    "- $\\beta_1$ is the coefficient for x\n",
    "\n",
    "Together, $\\beta_0$ and $\\beta_1$ are called the **model coefficients**. To create your model, you must \"estimate\" the values of these coefficients. And once we've estimated these parameters, we can use the model to predict things!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating Model Coefficients\n",
    "\n",
    "Generally speaking, coefficients are estimated using the **least squares criterion**, which means we are find the line (mathematically) which minimizes the **sum of squared residuals** (or \"sum of squared errors\"):\n",
    "\n",
    "<img src=\"Figures/estimating_coefficients.png\">\n",
    "\n",
    "What elements are present in the diagram?\n",
    "- The black dots are the **observed values** of x and y.\n",
    "- The blue line is our **least squares line**.\n",
    "- The red lines are the **residuals**, which are the distances between the observed values and the least squares line.\n",
    "\n",
    "How do the model coefficients relate to the least squares line?\n",
    "- $\\beta_0$ is the **intercept** (the value of $y$ when $x$=0)\n",
    "- $\\beta_1$ is the **slope** (the change in $y$ divided by change in $x$)\n",
    "\n",
    "Here is a graphical depiction of those calculations:\n",
    "\n",
    "<img src=\"Figures/slope_intercept.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run a regression we will be using the statsmodels module that was loaded above.  We will first initalize a model object with the regression equation and the data to use.  The format for specifying the regression equation is similar to R. (y ~ x).\n",
    "\n",
    "Here we will estimate the effect of gender on salary.  Gender is a \"dummy variable\", meaning that it is only ones and zeros.\n",
    "\n",
    "After we have initialized the object instance we can run the fit method (this can be chained so you only need to run one line).  After the parameters have been estimated we can query attributes of the model such as the estimated model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lm = smf.ols(formula = \"salary ~ gender\",data=df).fit()\n",
    "\n",
    "# print the coefficients\n",
    "print lm.params\n",
    "print lm.params[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting Model Coefficients\n",
    "\n",
    "How do we interpret the estimated coefficients for this model?\n",
    "- $\\beta_0$, or the intercept, reflects the average salary for the reference condition of the dummy code (i.e., when gender = 0).  This means that males make an average of `$69108.5`\n",
    "- to get the estimated female salary we need to add $\\beta_1$ (-13388.83), which becomes `$55719.67`\n",
    "\n",
    "We can also get a summary of all of the model statistics using the `summary()` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confidence in our Model\n",
    "\n",
    "Now let's examine the output. To learn more about Statsmodels and how to interpret the output, DataRobot has some decent posts on [simple linear regression](http://www.datarobot.com/blog/ordinary-least-squares-in-python/) and [multiple linear regression](http://www.datarobot.com/blog/multiple-regression-using-statsmodels/).\n",
    "\n",
    "\n",
    "**Question:** Is linear regression a high bias/low variance model, or a low bias/high variance model?\n",
    "\n",
    "**Answer:** High bias/low variance. Under repeated sampling, the line will stay roughly in the same place (low variance), but the average of those models won't do a great job capturing the true relationship (high bias). Note that low variance is a useful characteristic when you don't have a lot of training data!\n",
    "\n",
    "A closely related concept is **confidence intervals**. Statsmodels calculates 95% confidence intervals for our model coefficients, which are interpreted as follows: If the population from which this sample was drawn was **sampled 100 times**, approximately **95 of those confidence intervals** would contain the \"true\" coefficient.\n",
    "\n",
    "## Hypothesis Testing and p-values\n",
    "\n",
    "Closely related to confidence intervals is **hypothesis testing**. Generally speaking, you start with a **null hypothesis** and an **alternative hypothesis** (that is opposite the null). Then, you check whether the data supports **rejecting the null hypothesis** or **failing to reject the null hypothesis**.\n",
    "\n",
    "(Note that \"failing to reject\" the null is not the same as \"accepting\" the null hypothesis. The alternative hypothesis may indeed be true, except that you just don't have enough data to show that.)\n",
    "\n",
    "As it relates to model coefficients, here is the conventional hypothesis test:\n",
    "- **null hypothesis:** There is no relationship between gender and salary (and thus $\\beta_1$ equals zero)\n",
    "- **alternative hypothesis:** There is a relationship between TV ads and Sales (and thus $\\beta_1$ is not equal to zero)\n",
    "\n",
    "How do we test this hypothesis? Intuitively, we reject the null (and thus believe the alternative) if the 95% confidence interval **does not include zero**. Conversely, the **p-value** represents the probability that the coefficient is actually zero:\n",
    "\n",
    "## Overall Model Fit\n",
    "\n",
    "The overall goodness of how well the model fits the data can be described using the AIC or BIC which are information criterions that penalize for the number of free parameters in the model.  A common way to evaluate the overall fit of a linear model is **$R^2$** value. $R^2$ is the **proportion of variance explained**, meaning the proportion of variance in the observed data that is explained by the model, or the reduction in error over the **null model**. (The null model just predicts the mean of the observed response, and thus it has an intercept and no slope.)\n",
    "\n",
    "$R^2$ is between 0 and 1, and higher is better because it means that more variance is explained by the model. Here's an example of what R-squared \"looks like\":\n",
    "\n",
    "In the model above, we see that $R^2$=0.09, which means that gender does not do a great job at predicting the overall salary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Linear Regression\n",
    "\n",
    "Simple linear regression can easily be extended to include multiple features. This is called **multiple linear regression**:\n",
    "\n",
    "$y = \\beta_0 + \\beta_1x_1 + ... + \\beta_nx_n$\n",
    "\n",
    "Each $x$ represents a different feature, and each feature has its own coefficient. In this case:\n",
    "\n",
    "$Salary = \\beta_0 + \\beta_1 \\cdot Gender + \\beta_2 \\cdot Years$\n",
    "\n",
    "Let's use Statsmodels to estimate these coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lm2 = smf.ols(formula = \"salary ~ gender + years\",data=df).fit()\n",
    "lm2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that `years` also significantly explains additional variance.\n",
    "\n",
    "This is reflected in the $R^2$, which has now increased from 0.09 to 0.143. It is important to note that **$R^2$ will always increase as you add more features to the model**, even if they are unrelated to the response. Thus, selecting the model with the highest R-squared is not a reliable approach for choosing the best linear model.\n",
    "\n",
    "There is alternative to $R^2$ called **adjusted $R^2$** that penalizes model complexity (to control for overfitting), but it generally [under-penalizes complexity](http://scott.fortmann-roe.com/docs/MeasuringError.html).\n",
    "\n",
    "Next week we will explore another approach known as **Cross-validation.** Importantly, cross-validation can be applied to any model, whereas the methods described above only apply to linear models.\n",
    "\n",
    "Here, we will try and see if the new model significantly explains additional variance controlling for the extra free parameter using a nested model comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print sm.stats.anova_lm(lm,lm2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the addition of `years` in model2 significantly explains additional variance when compared to model 1. Notice that this is similar to the results provided by the t-tests on the individual parameters. Using model comparison to test incrementally adding a single predictor is a feature selection method known as stepwise regression.  However, you can also perform model comparison on models with many different features as long as one is nested within the other.\n",
    "\n",
    "When we only had one predictor we could investigate the relationship using a scatterplot.  Now that we have 2 predictors, we need to make a 3 dimensional plot to see the effect of each predictor on salary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure(figsize=(18,6))\n",
    "idx = [131,132,133]\n",
    "pos = {0:(-35,20),1:(-90,0),2:(0,0)}\n",
    "for i,ii in enumerate(idx):\n",
    "    ax = fig.add_subplot(ii, projection='3d')\n",
    "\n",
    "    # generate a mesh\n",
    "    x_surf = np.arange(0,11)/10.0\n",
    "    y_surf = np.arange(0,35,3)\n",
    "    x_surf, y_surf = np.meshgrid(x_surf, y_surf)\n",
    "\n",
    "    exog = pd.core.frame.DataFrame({'gender': x_surf.ravel(), 'years': y_surf.ravel()})\n",
    "    exog['intercept'] = np.ones(exog.shape[0])\n",
    "    out = lm2.predict(exog = exog,transform=True)\n",
    "\n",
    "    ax.plot_surface(x_surf, y_surf,\n",
    "                out.reshape(x_surf.shape),\n",
    "                rstride=1,\n",
    "                cstride=1,\n",
    "                color='None',\n",
    "                alpha = 0.2)\n",
    "\n",
    "    ax.scatter(df['gender'], df['years'], df['salary'],\n",
    "           c='blue',\n",
    "           marker='o',\n",
    "           alpha=1)\n",
    "\n",
    "    ax.set_xlabel('Gender')\n",
    "    ax.set_ylabel('Years')\n",
    "    ax.set_zlabel('Salary')\n",
    "    ax.azim = pos[i][0]\n",
    "    ax.elev = pos[i][1]\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how when we rotate the 3D plot, we can make projections into 2 dimensional space.  This is the effect of each predictor on salary controlling for the other.  \n",
    "\n",
    "We can also generate additional diagnostic plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ax = plt.figure(figsize=(12,10))\n",
    "sm.graphics.plot_regress_exog(lm2, 'years',fig=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we can create a standard ANOVA table with F statistics as if doing Anova.  F stat tests whether at least one of the coefficients of the variables of a category is significantly different from 0 or not.\n",
    "\n",
    "There are different times of Sum of Squared Error (SSQ) to consider.  We recommend using Type 3 by default.  See [here](https://mcfromnz.wordpress.com/2011/03/02/anova-type-iiiiii-ss-explained/) for more in depth discussion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Type 3 SSQ: valid for models with significant interaction terms\n",
    "print('ANOVA table for Type 3 SSQ')\n",
    "print sm.stats.anova_lm(lm2,typ=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources for Learning about Regression\n",
    "\n",
    "- To go much more in-depth on linear regression, read Chapter 3 of [An Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/). Alternatively, watch the [related videos](http://www.dataschool.io/15-hours-of-expert-machine-learning-videos/) or read a [quick reference guide](http://www.dataschool.io/applying-and-interpreting-linear-regression/) to the key points in that chapter.\n",
    "- This [introduction to linear regression](http://people.duke.edu/~rnau/regintro.htm) is much more detailed and mathematically thorough, and includes lots of good advice.\n",
    "- This is a relatively quick post on the [assumptions of linear regression](http://pareonline.net/getvn.asp?n=2&v=8)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Individual exercise\n",
    "### Use the 'salary_exercise.csv' to apply the analyses we learned today."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "This dataset was adapted from material available [here](http://data.princeton.edu/wws509/datasets/#salary)\n",
    "\n",
    "These are the salary data used in Weisberg's book, consisting of observations on six variables for 52 tenure-track professors in a small college. The variables are:\n",
    "\n",
    "- sx = Sex, coded 1 for female and 0 for male\n",
    "- rk = Rank, coded\n",
    " - 1 for assistant professor,\n",
    " - 2 for associate professor, and\n",
    " - 3 for full professor\n",
    "- yr = Number of years in current rank\n",
    "- dg = Highest degree, coded 1 if doctorate, 0 if masters\n",
    "- yd = Number of years since highest degree was earned\n",
    "- sl = Academic year salary, in dollars.\n",
    "\n",
    "Reference: S. Weisberg (1985). Applied Linear Regression, Second Edition. New York: John Wiley and Sons. Page 194."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Here are some sample questions to get you started\n",
    "1. Load the data 'salary_exercise.csv' into a pandas dataframe. \n",
    "2. Examine the data. Draw boxplots and scatterplots as you deem necessary. \n",
    "    Are there any missing data? Try cleaning and subsetting the data to use.\n",
    "3. Try a simple linear regression model with sex predicting salary. Are the coefficients significant? How would you interpret the results? \n",
    "4. What happens if you add other variables into your model? How would you compare the models? Which model fits best? \n",
    "5. ***Challenge*** Plot the least squares line for your simple linear regression. Consult the internet for help / ideas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
